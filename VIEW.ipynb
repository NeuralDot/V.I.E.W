{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIEW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj7rHG3khSRp",
        "colab_type": "code",
        "outputId": "a7e01574-6826-4154-8ce0-b2175e837b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Oct 10 07:15:35 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUvre32Um0K3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "b1f8a730-86d7-49b7-a555-1d1781dce6ca"
      },
      "source": [
        "!pip3 uninstall chainer\n",
        "!pip3 install chainer==1.24.0"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping chainer as it is not installed.\u001b[0m\n",
            "Collecting chainer==1.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/a4/e8cdcd04a607a72d39a7bea191f0ae952e941a53dd2a61ab851706babff3/chainer-1.24.0.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from chainer==1.24.0) (3.0.12)\n",
            "Collecting nose (from chainer==1.24.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer==1.24.0) (1.16.5)\n",
            "Requirement already satisfied: protobuf>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from chainer==1.24.0) (3.7.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer==1.24.0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=2.6.0->chainer==1.24.0) (41.2.0)\n",
            "Building wheels for collected packages: chainer\n",
            "  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chainer: filename=chainer-1.24.0-cp36-cp36m-linux_x86_64.whl size=4793968 sha256=8b6aaf48b0fd036a2efd15d27fc6012e37d8b300793bac73cea9a7c4f0fb80b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/79/8e/3eb03d4771f898f8063dd202876b9a8669e7cce8e27b90f20f\n",
            "Successfully built chainer\n",
            "Installing collected packages: nose, chainer\n",
            "Successfully installed chainer-1.24.0 nose-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g9SUKYYhU_T",
        "colab_type": "code",
        "outputId": "73381622-1529-40f1-eb35-1ff34b150d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install python3-pip\n",
        "!pip3 install scipy\n",
        "!pip3 install h5py\n",
        "!apt-get install python-h5py\n",
        "!apt-get install libopenjp2-7-dev\n",
        "!apt-get install libtiff5\n",
        "!pip3 install Pillow\n",
        "!apt-get install espeak\n",
        "!apt-get install python3-picamera\n",
        "!apt-get install libatlas-base-dev"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto\n",
            "  python3-cryptography python3-idna python3-keyring python3-keyrings.alt\n",
            "  python3-pkg-resources python3-secretstorage python3-setuptools python3-six\n",
            "  python3-wheel python3-xdg\n",
            "Suggested packages:\n",
            "  python-crypto-doc python-cryptography-doc python3-cryptography-vectors\n",
            "  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0\n",
            "  python-secretstorage-doc python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto\n",
            "  python3-cryptography python3-idna python3-keyring python3-keyrings.alt\n",
            "  python3-pip python3-pkg-resources python3-secretstorage python3-setuptools\n",
            "  python3-six python3-wheel python3-xdg\n",
            "0 upgraded, 15 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 2,883 kB of archives.\n",
            "After this operation, 8,884 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.1 [1,653 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-asn1crypto all 0.24.0-1 [72.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-cffi-backend amd64 1.11.5-1 [64.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-cryptography amd64 2.1.4-1ubuntu1.3 [221 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-secretstorage all 2.3.1-2 [12.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyring all 10.6.0-1 [26.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyrings.alt all 3.0-1 [16.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-pip all 9.0.1-2.3~ubuntu1.18.04.1 [114 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-setuptools all 39.0.1-2 [248 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-wheel all 0.30.0-0.2 [36.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-xdg all 0.25-4ubuntu1 [31.4 kB]\n",
            "Fetched 2,883 kB in 12s (237 kB/s)\n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.1_all.deb ...\n",
            "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Selecting previously unselected package python3-asn1crypto.\n",
            "Preparing to unpack .../01-python3-asn1crypto_0.24.0-1_all.deb ...\n",
            "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
            "Selecting previously unselected package python3-cffi-backend.\n",
            "Preparing to unpack .../02-python3-cffi-backend_1.11.5-1_amd64.deb ...\n",
            "Unpacking python3-cffi-backend (1.11.5-1) ...\n",
            "Selecting previously unselected package python3-crypto.\n",
            "Preparing to unpack .../03-python3-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
            "Unpacking python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Selecting previously unselected package python3-idna.\n",
            "Preparing to unpack .../04-python3-idna_2.6-1_all.deb ...\n",
            "Unpacking python3-idna (2.6-1) ...\n",
            "Selecting previously unselected package python3-six.\n",
            "Preparing to unpack .../05-python3-six_1.11.0-2_all.deb ...\n",
            "Unpacking python3-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python3-cryptography.\n",
            "Preparing to unpack .../06-python3-cryptography_2.1.4-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking python3-cryptography (2.1.4-1ubuntu1.3) ...\n",
            "Selecting previously unselected package python3-secretstorage.\n",
            "Preparing to unpack .../07-python3-secretstorage_2.3.1-2_all.deb ...\n",
            "Unpacking python3-secretstorage (2.3.1-2) ...\n",
            "Selecting previously unselected package python3-keyring.\n",
            "Preparing to unpack .../08-python3-keyring_10.6.0-1_all.deb ...\n",
            "Unpacking python3-keyring (10.6.0-1) ...\n",
            "Selecting previously unselected package python3-keyrings.alt.\n",
            "Preparing to unpack .../09-python3-keyrings.alt_3.0-1_all.deb ...\n",
            "Unpacking python3-keyrings.alt (3.0-1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../10-python3-pip_9.0.1-2.3~ubuntu1.18.04.1_all.deb ...\n",
            "Unpacking python3-pip (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../11-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../12-python3-setuptools_39.0.1-2_all.deb ...\n",
            "Unpacking python3-setuptools (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../13-python3-wheel_0.30.0-0.2_all.deb ...\n",
            "Unpacking python3-wheel (0.30.0-0.2) ...\n",
            "Selecting previously unselected package python3-xdg.\n",
            "Preparing to unpack .../14-python3-xdg_0.25-4ubuntu1_all.deb ...\n",
            "Unpacking python3-xdg (0.25-4ubuntu1) ...\n",
            "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Setting up python3-cffi-backend (1.11.5-1) ...\n",
            "Setting up python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Setting up python3-idna (2.6-1) ...\n",
            "Setting up python3-xdg (0.25-4ubuntu1) ...\n",
            "Setting up python3-six (1.11.0-2) ...\n",
            "Setting up python3-wheel (0.30.0-0.2) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up python3-asn1crypto (0.24.0-1) ...\n",
            "Setting up python3-pip (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up python3-setuptools (39.0.1-2) ...\n",
            "Setting up python3-cryptography (2.1.4-1ubuntu1.3) ...\n",
            "Setting up python3-keyrings.alt (3.0-1) ...\n",
            "Setting up python3-secretstorage (2.3.1-2) ...\n",
            "Setting up python3-keyring (10.6.0-1) ...\n",
            "Collecting chainer==1.19.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b9/7926fb395a1af409ca4730387cd8d729e24dfe0e7e150c21c138b9f2bf2d/chainer-1.19.0.tar.gz (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 3.3MB/s \n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.16.5)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python-six\n",
            "Suggested packages:\n",
            "  python-h5py-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-h5py python-six\n",
            "0 upgraded, 2 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 634 kB of archives.\n",
            "After this operation, 2,775 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-six all 1.11.0-2 [11.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-h5py amd64 2.7.1-2 [623 kB]\n",
            "Fetched 634 kB in 1s (485 kB/s)\n",
            "Selecting previously unselected package python-six.\n",
            "(Reading database ... 131863 files and directories currently installed.)\n",
            "Preparing to unpack .../python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-h5py.\n",
            "Preparing to unpack .../python-h5py_2.7.1-2_amd64.deb ...\n",
            "Unpacking python-h5py (2.7.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-h5py (2.7.1-2) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libopenjp2-7-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 26.6 kB of archives.\n",
            "After this operation, 166 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libopenjp2-7-dev amd64 2.3.0-2build0.18.04.1 [26.6 kB]\n",
            "Fetched 26.6 kB in 0s (72.7 kB/s)\n",
            "Selecting previously unselected package libopenjp2-7-dev.\n",
            "(Reading database ... 131954 files and directories currently installed.)\n",
            "Preparing to unpack .../libopenjp2-7-dev_2.3.0-2build0.18.04.1_amd64.deb ...\n",
            "Unpacking libopenjp2-7-dev (2.3.0-2build0.18.04.1) ...\n",
            "Setting up libopenjp2-7-dev (2.3.0-2build0.18.04.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libtiff5 is already the newest version (4.0.9-5ubuntu0.2).\n",
            "libtiff5 set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow) (0.46)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak espeak-data libespeak1 libportaudio2 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 1,219 kB of archives.\n",
            "After this operation, 3,031 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-data amd64 1.48.04+dfsg-5 [934 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libespeak1 amd64 1.48.04+dfsg-5 [145 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak amd64 1.48.04+dfsg-5 [61.6 kB]\n",
            "Fetched 1,219 kB in 3s (452 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 131972 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-6) ...\n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "Preparing to unpack .../espeak-data_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../libespeak1_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package espeak.\n",
            "Preparing to unpack .../espeak_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak (1.48.04+dfsg-5) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-6) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up espeak (1.48.04+dfsg-5) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package python3-picamera\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libatlas-base-dev is already the newest version (3.10.3-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6exILJJ9hhrx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "52e6bbdb-6a8a-4859-91e4-4ceb639371e9"
      },
      "source": [
        "!apt-get install git\n",
        "!git clone https://github.com/apple2373/chainer-caption.git\n",
        "%cd chainer-caption"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "Cloning into 'chainer-caption'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 363 (delta 10), reused 0 (delta 0), pack-reused 345\u001b[K\n",
            "Receiving objects: 100% (363/363), 2.99 MiB | 19.98 MiB/s, done.\n",
            "Resolving deltas: 100% (223/223), done.\n",
            "bash: download.sh: No such file or directory\n",
            "python3: can't open file 'sample_code_beam.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44NqDYqusF2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## After uploading external image im2\n",
        "!mv /content/im2.jpg /content/chainer-caption/im2.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-byGrHKijsw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "55c2f5df-91c2-4122-d74c-1139d7f6cb33"
      },
      "source": [
        "!bash download.sh\n",
        "!python3 sample_code_beam.py --rnn-model ./data/caption_en_model40.model --cnn-model ./data/ResNet50.model --vocab ./data/MSCOCO/mscoco_caption_train2014_processed_dic.json --gpu -1 --img ./im2.jpg"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> a traffic light sitting on top of a pole <eos>\n",
            "-8.06228917883709\n",
            "<sos> a traffic light sitting on top of a street <eos>\n",
            "-9.018726706970483\n",
            "<sos> a traffic light sitting on top of a tall building <eos>\n",
            "-9.246615365613252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HXblKbhipWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0a2109e0-9ab7-4e1f-a924-4af5c77be7d5"
      },
      "source": [
        "# !wget https://raw.githubusercontent.com/yoshihiroo/programming-workshop/master/image_captioning_and_speech/image_captioning.py"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-10 07:51:25--  https://raw.githubusercontent.com/yoshihiroo/programming-workshop/master/image_captioning_and_speech/image_captioning.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1779 (1.7K) [text/plain]\n",
            "Saving to: ‘image_captioning.py’\n",
            "\n",
            "\rimage_captioning.py   0%[                    ]       0  --.-KB/s               \rimage_captioning.py 100%[===================>]   1.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-10-10 07:51:26 (309 MB/s) - ‘image_captioning.py’ saved [1779/1779]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3Yafg1Lpo0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python3 image_captioning.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rVAh-YWBtZNW",
        "colab": {}
      },
      "source": [
        "##### sample_code_beam.py ######\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "'''\n",
        "Sample code to generate caption using beam search\n",
        "'''\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "# comment out the below if you want to do type check. Remeber this have to be done BEFORE import chainer\n",
        "# os.environ[\"CHAINER_TYPE_CHECK\"] = \"0\"\n",
        "import chainer \n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import math\n",
        "from chainer import cuda\n",
        "import chainer.functions as F\n",
        "from chainer import cuda, Function, gradient_check, Variable, optimizers\n",
        "from chainer import serializers\n",
        "\n",
        "sys.path.append('./code')\n",
        "from CaptionGenerator import CaptionGenerator\n",
        "\n",
        "#Parse arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-g\", \"--gpu\",default=-1, type=int, help=u\"GPU ID.CPU is -1\")\n",
        "parser.add_argument('--vocab',default='./data/MSCOCO/mscoco_caption_train2014_processed_dic.json', type=str,help='path to the vocaburary json')\n",
        "parser.add_argument('--img',default='./sample_imgs/dog.jpg', type=str,help='path to the image')\n",
        "parser.add_argument('--cnn-model', type=str, default='./data/ResNet50.model',help='place of the ResNet model')\n",
        "parser.add_argument('--rnn-model', type=str, default='./data/caption_model.model',help='place of the caption model')\n",
        "parser.add_argument('--beam',default=3, type=int,help='beam size in beam search')\n",
        "parser.add_argument('--depth',default=50, type=int,help='depth limit in beam search')\n",
        "parser.add_argument('--lang',default=\"<sos>\", type=str,help='special word to indicate the langauge or just <sos>')\n",
        "args = parser.parse_args()\n",
        "\n",
        "caption_generator=CaptionGenerator(\n",
        "    rnn_model_place=args.rnn_model,\n",
        "    cnn_model_place=args.cnn_model,\n",
        "    dictonary_place=args.vocab,\n",
        "    beamsize=args.beam,\n",
        "    depth_limit=args.depth,\n",
        "    gpu_id=args.gpu,\n",
        "    first_word= args.lang,\n",
        "    )\n",
        "\n",
        "captions = caption_generator.generate(args.img)\n",
        "for caption in captions:\n",
        "    print (\" \".join(caption[\"sentence\"]))\n",
        "    print (caption[\"log_likelihood\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YLyWjuIrZvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### code/CaptionGenerator.py #######\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "#!/usr/bin/env python\n",
        "\n",
        "'''\n",
        "caption generation module by beam search\n",
        "currently you cannot use efficient generation by batch. Batch size is always one \n",
        "\n",
        "beam search might have a small bug or not the most efficient, but seems to be ok.\n",
        "'''\n",
        "\n",
        "import os\n",
        "import chainer \n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "from copy import deepcopy\n",
        "\n",
        "from chainer import cuda\n",
        "import chainer.functions as F\n",
        "from chainer import cuda, Function, gradient_check, Variable, optimizers\n",
        "from chainer import serializers\n",
        "\n",
        "from image_loader import Image_loader\n",
        "from ResNet50 import ResNet\n",
        "from Image2CaptionDecoder import Image2CaptionDecoder\n",
        "\n",
        "#priority queue\n",
        "#reference: http://www.bogotobogo.com/python/python_PriorityQueue_heapq_Data_Structure.php\n",
        "try:\n",
        "    import Queue as Q  # ver. < 3.0\n",
        "except ImportError:\n",
        "    import queue as Q\n",
        "\n",
        "import heapq\n",
        "\n",
        "class CaptionGenerator(object):\n",
        "    def __init__(self,rnn_model_place,cnn_model_place,dictonary_place,beamsize=3,depth_limit=50,gpu_id=-1,first_word=\"<sos>\",hidden_dim=512,mean=\"imagenet\"):\n",
        "        self.gpu_id=gpu_id\n",
        "        self.beamsize=beamsize\n",
        "        self.depth_limit=depth_limit\n",
        "        self.image_loader=Image_loader(mean)\n",
        "        self.index2token=self.parse_dic(dictonary_place)\n",
        "\n",
        "        self.cnn_model=ResNet()\n",
        "        serializers.load_hdf5(cnn_model_place, self.cnn_model)\n",
        "        self.cnn_model.train = False\n",
        "\n",
        "        self.rnn_model=Image2CaptionDecoder(len(self.token2index),hidden_dim=hidden_dim)\n",
        "        if len(rnn_model_place) > 0:\n",
        "            serializers.load_hdf5(rnn_model_place, self.rnn_model)\n",
        "        self.rnn_model.train = False\n",
        "\n",
        "        self.first_word=first_word\n",
        "\n",
        "        #Gpu Setting\n",
        "        global xp\n",
        "        if self.gpu_id >= 0:\n",
        "            xp = cuda.cupy \n",
        "            cuda.get_device(gpu_id).use()\n",
        "            self.cnn_model.to_gpu()\n",
        "            self.rnn_model.to_gpu()\n",
        "        else:\n",
        "            xp=np\n",
        "\n",
        "    def parse_dic(self,dictonary_place):\n",
        "        with open(dictonary_place, 'r') as f:\n",
        "            json_file = json.load(f)\n",
        "        if len(json_file) < 10:#this is ad-hock. I need to distinguish new format and old format...\n",
        "            self.token2index = { word['word']:word['idx'] for word in json_file[\"words\"]}\n",
        "        else:\n",
        "            self.token2index = json_file\n",
        "\n",
        "        return {v:k for k,v in self.token2index.items()}\n",
        "\n",
        "    def successor(self,current_state):\n",
        "        '''\n",
        "        Args:\n",
        "            current_state: a stete, python tuple (hx,cx,path,cost)\n",
        "                hidden: hidden states of LSTM\n",
        "                cell: cell states LSTM\n",
        "                path: word indicies so far as a python list  e.g. initial is self.token2index[\"<sos>\"]\n",
        "                cost: negative log likelihood\n",
        "\n",
        "        Returns:\n",
        "            k_best_next_states: a python list whose length is the beam size. possible_sentences[i] = {\"indicies\": list of word indices,\"cost\":negative log likelihood so far}\n",
        "\n",
        "        '''\n",
        "\n",
        "        word=[xp.array([current_state[\"path\"][-1]],dtype=xp.int32)]\n",
        "        hx=current_state[\"hidden\"]\n",
        "        cx=current_state[\"cell\"]\n",
        "        hy, cy, next_words=self.rnn_model(hx,cx,word)\n",
        "\n",
        "        word_dist=F.softmax(next_words[0]).data[0]#possible next word distributions\n",
        "        k_best_next_sentences=[]\n",
        "        for i in range(self.beamsize):\n",
        "            next_word_idx=int(xp.argmax(word_dist))\n",
        "            k_best_next_sentences.append(\\\n",
        "                {\\\n",
        "                \"hidden\":hy,\\\n",
        "                \"cell\":cy,\\\n",
        "                \"path\":deepcopy(current_state[\"path\"])+[next_word_idx],\\\n",
        "                \"cost\":current_state[\"cost\"]-xp.log(word_dist[next_word_idx])\n",
        "                }\\\n",
        "                )\n",
        "            word_dist[next_word_idx]=0\n",
        "\n",
        "        return hy, cy, k_best_next_sentences\n",
        "\n",
        "    def beam_search(self,initial_state):\n",
        "        '''\n",
        "        Beam search is a graph search algorithm! So I use graph search abstraction\n",
        "\n",
        "        Args:\n",
        "            initial state: an initial stete, python tuple (hx,cx,path,cost)\n",
        "            each state has \n",
        "                hx: hidden states\n",
        "                cx: cell states\n",
        "                path: word indicies so far as a python list  e.g. initial is self.token2index[\"<sos>\"]\n",
        "                cost: negative log likelihood\n",
        "\n",
        "        Returns:\n",
        "            captions sorted by the cost (i.e. negative log llikelihood)\n",
        "        '''\n",
        "        found_paths=[]\n",
        "        top_k_states=[initial_state]\n",
        "        while (len(found_paths) < self.beamsize):\n",
        "            #forward one step for all top k states, then only select top k after that\n",
        "            new_top_k_states=[]\n",
        "            for state in top_k_states:\n",
        "                #examine to next five possible states\n",
        "                hy, cy, k_best_next_states = self.successor(state)\n",
        "                for next_state in k_best_next_states:\n",
        "                    new_top_k_states.append(next_state)\n",
        "            selected_top_k_states=heapq.nsmallest(self.beamsize, new_top_k_states, key=lambda x : x[\"cost\"])\n",
        "\n",
        "            #within the selected states, let's check if it is terminal or not.\n",
        "            top_k_states=[]\n",
        "            for state in selected_top_k_states:\n",
        "                #is goal state? -> yes, then end the search\n",
        "                if state[\"path\"][-1] == self.token2index[\"<eos>\"] or len(state[\"path\"])==self.depth_limit:\n",
        "                    found_paths.append(state)\n",
        "                else:\n",
        "                    top_k_states.append(state)\n",
        "\n",
        "        return sorted(found_paths, key=lambda x: x[\"cost\"]) \n",
        "\n",
        "    def beam_search0(self,initial_state):\n",
        "        #original one. This takes much memory\n",
        "        '''\n",
        "        Beam search is a graph search algorithm! So I use graph search abstraction\n",
        "        Args:\n",
        "            initial state: an initial stete, python tuple (hx,cx,path,cost)\n",
        "            each state has \n",
        "                hx: hidden states\n",
        "                cx: cell states\n",
        "                path: word indicies so far as a python list  e.g. initial is self.token2index[\"<sos>\"]\n",
        "                cost: negative log likelihood\n",
        "        Returns:\n",
        "            captions sorted by the cost (i.e. negative log llikelihood)\n",
        "        '''\n",
        "        found_paths=[]\n",
        "        q = Q.PriorityQueue()\n",
        "        q.put((0,initial_state))\n",
        "        while (len(found_paths) < self.beamsize):\n",
        "            i=0\n",
        "            # this is just a one step ahead? \n",
        "            while not q.empty():\n",
        "                if i == self.beamsize:\n",
        "                    break\n",
        "                state=q.get()[1]\n",
        "                #is goal state? -> yes, then end the search\n",
        "                if state[\"path\"][-1] == self.token2index[\"<eos>\"] or len(state[\"path\"])==self.depth_limit:\n",
        "                    found_paths.append(state)\n",
        "                    continue\n",
        "                #examine to next five possible states and add to priority queue \n",
        "                hy, cy, k_best_next_states = self.successor(state)\n",
        "                for next_state in k_best_next_states:\n",
        "                    q.put((state[\"cost\"],next_state))\n",
        "                i+=1\n",
        "\n",
        "        return sorted(found_paths, key=lambda x: x[\"cost\"]) \n",
        "\n",
        "    def generate(self,image_file_path):\n",
        "        '''\n",
        "        Args:\n",
        "            image_file_path: image_file_path\n",
        "        '''\n",
        "        img=self.image_loader.load(image_file_path)\n",
        "        return self.generate_from_img(img)\n",
        "\n",
        "\n",
        "    def generate_from_img_feature(self,image_feature):\n",
        "        if self.gpu_id >= 0:\n",
        "            image_feature=cuda.to_gpu(image_feature)\n",
        "\n",
        "        batch_size=1\n",
        "        hx=xp.zeros((self.rnn_model.n_layers, batch_size, self.rnn_model.hidden_dim), dtype=xp.float32)\n",
        "        cx=xp.zeros((self.rnn_model.n_layers, batch_size, self.rnn_model.hidden_dim), dtype=xp.float32)\n",
        "        \n",
        "        hy,cy = self.rnn_model.input_cnn_feature(hx,cx,image_feature)\n",
        "\n",
        "\n",
        "        initial_state={\\\n",
        "                    \"hidden\":hy,\\\n",
        "                    \"cell\":cy,\\\n",
        "                    \"path\":[self.token2index[self.first_word]],\\\n",
        "                    \"cost\":0,\\\n",
        "                }\\\n",
        "\n",
        "        captions=self.beam_search(initial_state)\n",
        "\n",
        "        caption_candidates=[]\n",
        "        for caption in captions:\n",
        "            sentence= [self.index2token[word_idx] for word_idx in caption[\"path\"]]\n",
        "            log_likelihood = -float(caption[\"cost\"])#cost is the negative log likelihood\n",
        "            caption_candidates.append({\"sentence\":sentence,\"log_likelihood\":log_likelihood})\n",
        "\n",
        "        return caption_candidates\n",
        "\n",
        "    def generate_from_img(self,image_array):\n",
        "        '''Generate Caption for an Numpy Image array\n",
        "        \n",
        "        Args:\n",
        "            image_array: numpy array of image\n",
        "\n",
        "        Returns:\n",
        "            list of generated captions, sorted by the cost (i.e. negative log llikelihood)\n",
        "\n",
        "            The structure is [caption,caption,caption,...]\n",
        "            Where caption = {\"sentence\": a generated sentence as a python list of word, \"log_likelihood\": The log llikelihood of the generated sentence} \n",
        "\n",
        "        '''\n",
        "        if self.gpu_id >= 0:\n",
        "            image_array=cuda.to_gpu(image_array)\n",
        "        image_feature=self.cnn_model(image_array, \"feature\").data.reshape(1,1,2048)#次元が一つ多いのは、NstepLSTMはsequaenceとみなすから。(sequence size, batch size, feature dim)ということ\n",
        "\n",
        "        return self.generate_from_img_feature(image_feature)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    xp=np\n",
        "    #test code on cpu\n",
        "    caption_generator=CaptionGenerator(\n",
        "        rnn_model_place=\"../experiment1/caption_model1.model\",\\\n",
        "        cnn_model_place=\"../data/ResNet50.model\",\\\n",
        "        dictonary_place=\"../data/MSCOCO/mscoco_caption_train2014_processed_dic.json\",\\\n",
        "        beamsize=3,depth_limit=50,gpu_id=-1,)\n",
        "\n",
        "    # batch_size=1\n",
        "    # hx=xp.zeros((caption_generator.rnn_model.n_layers, batch_size, caption_generator.rnn_model.hidden_dim), dtype=xp.float32)\n",
        "    # cx=xp.zeros((caption_generator.rnn_model.n_layers, batch_size, caption_generator.rnn_model.hidden_dim), dtype=xp.float32)\n",
        "    # img=caption_generator.image_loader.load(\"../sample_imgs/COCO_val2014_000000185546.jpg\")\n",
        "    # image_feature=caption_generator.cnn_model(img, \"feature\").data.reshape(1,1,2048)#次元が一つ多いのは、NstepLSTMはsequaenceとみなすから。(sequence size, batch size, feature dim)ということ\n",
        "    \n",
        "    # hy,cy = caption_generator.rnn_model.input_cnn_feature(hx,cx,image_feature)\n",
        "    # initial_state={\\\n",
        "    #             \"hidden\":hy,\\\n",
        "    #             \"cell\":cy,\\\n",
        "    #             \"path\":[caption_generator.token2index[\"<sos>\"]],\\\n",
        "    #             \"cost\":0,\\\n",
        "    #             }\\\n",
        "\n",
        "    # #successor test\n",
        "    # next_states= caption_generator.successor(initial_state)\n",
        "    # print next_states\n",
        "\n",
        "    # print \"beam search test\"\n",
        "    # captions= caption_generator.beam_search(initial_state)\n",
        "    # print captions\n",
        "\n",
        "    captions = caption_generator.generate(\"../sample_imgs/COCO_val2014_000000185546.jpg\")\n",
        "    for caption in captions:\n",
        "        print(caption[\"sentence\"])\n",
        "        print(caption[\"log_likelihood\"])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}